{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "import psycopg2\n",
    "book_url = 'https://tiki.vn/nha-sach-tiki/c8322?src=c.8322.hamburger_menu_fly_out_banner'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class categories():\n",
    "#     def __init__(self, id_, name, url, parent_id):\n",
    "#         self.id_ = id_\n",
    "#         self.name = name\n",
    "#         self.url = url\n",
    "#         self.parent_id = parent_id\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting data into readable html format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get detail information after converting html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_item(item):\n",
    "#     ms = item['data-id']\n",
    "#     name = item['data-title'].strip(' \"')\n",
    "#     category = item['data-category']\n",
    "#     brand = item['data-brand']\n",
    "#     price = item['data-price']\n",
    "#     img = item.img['src']\n",
    "#     url = item.a['href']\n",
    "#     rating = (item.find('span', class_='rating-content').span['style'][-4:].strip(' %:') if item.find('span', class_='rating-content')!= None else -1)\n",
    "#     tikinow = (1 if item.find('i', class_='tikicon icon-tikinow') else 0)\n",
    "#     res = item(id_ = id_,\n",
    "#                name= name,\n",
    "#                category= category, brand= brand, \n",
    "#                img= img,price= price, \n",
    "#                url= url,rating= rating,\n",
    "#                tikinow= tikinow)\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ham nay nghia la gi?????\n",
    "res = Item(id_ = id_,\n",
    "               name= name,\n",
    "               category= category, brand= brand, \n",
    "               img_link= img_link,price= price, \n",
    "               item_link= item_link,rating= rating,\n",
    "               no_of_review= review, tikinow= tikinow)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conect to database\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create table function\n",
    "conn = psycopg2.connect(user = 'khanh', database = 'khanhdb')\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_table():\n",
    "#     query = '''\n",
    "#     CREATE TABLE IF NOT EXISTS product (\n",
    "#     id SERIAL PRIMARY KEY,\n",
    "#     name VARCHAR(255),\n",
    "#     category VARCHAR(255), \n",
    "#     brand VARCHAR (255),\n",
    "#     img TEXT,\n",
    "#     price INT,\n",
    "#     url TEXT, \n",
    "#     rating TEXT,\n",
    "#     tikinow TEXT);\n",
    "#     '''\n",
    "#     try:\n",
    "#         cur.execute(query)\n",
    "#     except Exception as err:\n",
    "#         print(f'ERROR: {err}')\n",
    "\n",
    "# create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = '''\n",
    "# ALTER TABLE product RENAME brand TO \n",
    "# '''\n",
    "# cur.execute(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class item():\n",
    "#     def __init__(self, ms, name, category, \n",
    "#                  brand, img, price, url, \n",
    "#                  rating, tikinow):\n",
    "#         self.ms = ms\n",
    "#         self.name = name\n",
    "#         self.category = category\n",
    "#         self.brand = brand\n",
    "#         self.img = img\n",
    "#         self.price = price\n",
    "#         self.url = url\n",
    "#         self.rating = rating\n",
    "#         self.tikinow = tikinow\n",
    "    \n",
    "#     def save_into_db(self):\n",
    "#         query = '''\n",
    "#         INSERT INTO product (ms, name, category, \n",
    "#             brand, img, price, url, rating, tikinow)\n",
    "#         VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING ms;\n",
    "#         '''\n",
    "#         val = (self.ms, self.name, self.category, self.brand,\n",
    "#         self.img, self.price, self.url, self.rating,\n",
    "#         self.tikinow)\n",
    "#         try:\n",
    "#             cur.execute(query, val)\n",
    "#         except Exception as err:\n",
    "#             print(f'Error:{err}')\n",
    "        \n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         return f'''[ms: {self.ms}, name: {self.name},\\\n",
    "#                 category: {self.category}, brand: {self.brand},\\ \n",
    "#                 img: {self.img}, price: {self.price},\\ \n",
    "#                 url: {self.url}, rating: {self.rating},\\ \n",
    "#                 tikinow: {('Yes' if self.tikinow else 'No') }]'''\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the information from 1 site\n",
    "def parser(url):\n",
    "    try: \n",
    "        res = requests.get(url)\n",
    "        res = BeautifulSoup(res.text)\n",
    "    except Exception as err:\n",
    "        print('error')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # testing function\n",
    "# # s = parse(url)\n",
    "# # s.findAll('div',{'class': 'product-item'})\n",
    "# i = s.findAll('div',{'class': 'product-item'})\n",
    "# a = i[0]\n",
    "# id_ = a['data-id']\n",
    "# name = a['data-title'].strip(' \"')\n",
    "# category = a['data-category']\n",
    "# brand = a['data-brand']\n",
    "# price = a['data-price']\n",
    "# img = a.img['src']\n",
    "# url = a.a['href']\n",
    "# tikinow = (1 if a.find('i', class_='tikicon icon-tikinow') else -1)\n",
    "        \n",
    "# print(id_,name,category, brand, price, img, url, tikinow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_info(save_db=False):\n",
    "#     #run parser\n",
    "#     s = parser(book_url)\n",
    "#     #create an empty list\n",
    "#     info_list = []\n",
    "    \n",
    "#     #scrape through page\n",
    "#     for i in s.findAll('div',{'class': 'product-item'}):\n",
    "#         ms = i['data-id']\n",
    "#         name = i['data-title']\n",
    "#         category = i['data-category']\n",
    "#         brand = i['data-brand']\n",
    "#         price = i['data-price']\n",
    "#         img = i.img['src']\n",
    "#         url = i.a['href']\n",
    "#         rating = (i.find('span', class_='rating-content').span['style'][-4:].strip(' %:') if i.find('span', class_='rating-content')!= None else -1)\n",
    "#         tikinow = (1 if i.find('i', class_='tikicon icon-tikinow') else -1)\n",
    "        \n",
    "#         products = item(ms, name, category, \n",
    "#                  brand, img, price, url, \n",
    "#                  rating, tikinow)\n",
    "#         if save_db:\n",
    "#             products.save_into_db()\n",
    "#         info_list.append(item)\n",
    "\n",
    "#     return info_list\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info = get_info(save_db=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.DataFrame({'id':id_list, \n",
    "#                   'name':name_list, \n",
    "#                   'category':category_list, \n",
    "#                   'brand':brand_list, \n",
    "#                   'item_link':item_link_list, \n",
    "#                   'img_link':img_link_list, \n",
    "#                   'price':price_list,\n",
    "#                   'rating':rating_list,\n",
    "#                   'review':no_review_list,\n",
    "#                   'tikinow':tikinow_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the information from many sites\n",
    "def load_website(url, prefix):\n",
    "    try:\n",
    "        res = requests.get(url+prefix)\n",
    "        return BeautifulSoup(res.text)\n",
    "    except Exception as err:\n",
    "        print (f'Error: {err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#get infor from all Tiki page\n",
    "soup = load_website('https://tiki.vn/',prefix='')\n",
    "\n",
    "# scrape and store Tiki cate\n",
    "category = soup.findAll('a', {'class':'MenuItem__MenuLink-tii3xq-1 efuIbv'})\n",
    "category_list, link = [], []\n",
    "for h in range (len(category)):\n",
    "    try:\n",
    "        link.append(category[h]['href'])\n",
    "        category_list.append(category[h].text)\n",
    "    except:\n",
    "        print('pass')\n",
    "cat = list(zip(category_list, link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = soup.findAll('a', {'class':'MenuItem__MenuLink-tii3xq-1 efuIbv'})\n",
    "category[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new table\n",
    "tablename = 'Tiki_product'\n",
    "query = f'''\n",
    "    CREATE TABLE {tablename}\n",
    "    (id SERIAL PRIMARY KEY, img VARCHAR(1024), \n",
    "    title VARCHAR(1024),\n",
    "    fprice VARCHAR(1024), rprice VARCHAR(1024), \n",
    "    discount VARCHAR(1024), cate VARCHAR(1024), \n",
    "    subcate VARCHAR(1024), rating VARCHAR(1024),\n",
    "    tikinow VARCHAR(1024), link VARCHAR(1024)\n",
    "   );\n",
    "    '''\n",
    "\n",
    "cur.execute(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_insert(cat, j, articles, k, cur, conn, table):\n",
    "    #get information from Tiki\n",
    "    try:\n",
    "        img = articles[k].img['src']\n",
    "        fprice = articles[k].find_all(\"span\",{\"class\":\"final-price\"})[0].text.strip().split()[0]\n",
    "        rprice = articles[k].find_all(\"span\",{\"class\":\"price-regular\"})[0].text\n",
    "        discount = ['None' if len(articles[k].find_all(\"span\",{\"class\":\"final-price\"})[0].text.strip().split()) == 1 else articles[k].find_all(\"span\",{\"class\":\"final-price\"})[0].text.strip().split()[1]][0]\n",
    "        title = articles[k].a['title'].strip().replace('\\'','').replace('\"','')\n",
    "        subcategory = articles[k]['data-category'].strip()\n",
    "        cate = cat[j][0]\n",
    "        ratings = [articles[k].find_all('span',{\"class\":'rating-content'})[0].find('span')['style'].split(':')[1] if articles[k].find_all('span',{\"class\":'rating-content'}) != [] else 'Rating not available'][0]\n",
    "        tikinow = ['NO' if articles[k].find_all('i',{\"class\":\"tikicon icon-tikinow-20\"}) == [] else 'YES'][0]\n",
    "        link = articles[k].a['href']\n",
    "        \n",
    "        # query string\n",
    "        query = f'''\n",
    "        INSERT INTO {tablename} (images, fprice, category, subcategory, titles, rprice, discount, ratings, tikinow, link)\n",
    "        VALUES('{images}', '{fprice}', '{category}','{subcategory}', '{titles}','{rprice}','{discount}','{ratings}','{tikinow}', '{link}');\n",
    "        '''\n",
    "        # commit to database\n",
    "        cur.execute(query)\n",
    "        conn.commit\n",
    "    except Exception as err:\n",
    "        print('err')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(cat)):\n",
    "    try:\n",
    "        soup = load_website(cat[j][1],prefix='')\n",
    "        articles = soup.find_all('div', {\"class\":'product-item'})\n",
    "        print('reading'+cat[j][1])\n",
    "    except Exception as Err:\n",
    "        print('pass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j in range(len(cat)):\n",
    "#     try:\n",
    "#         soup = load_website(cat[j][1],prefix='')\n",
    "#         articles = soup.find_all('div', {\"class\":'product-item'})\n",
    "#         print('reading'+cat[j][1])\n",
    "#         for k in range (len(articles)):\n",
    "#             scrape_insert(cat, j, articles, k, cur, conn, tablename)\n",
    "#         # Read next page cursor at the bottom of a product page        \n",
    "#             links = soup.find_all('div',{\"class\":'list-pager'})  \n",
    "\n",
    "#             #While next page cursor is not empty, read next page cursor to move to next product page\n",
    "#         while links[0].find_all('a', {\"class\": \"next\"}) != []:\n",
    "#             try:\n",
    "#                 soup = load_website(links[0].find_all('a', {\"class\": \"next\"})[0]['href'],prefix='https://tiki.vn')\n",
    "#                 articles = soup.find_all('div', {\"class\":\"product-item\"})\n",
    "#                 print('Reading',cat[j][0],links[0].find_all('a', {\"class\": \"next\"})[0]['href'].split('&')[1],sep=' ')\n",
    "#                 for i in range(len(articles)):\n",
    "#                     scrape_and_insert(cat, j, articles, i, cur, conn, tablename)\n",
    "#                 links = soup.find_all('div',{\"class\":'list-pager'})\n",
    "#             except:\n",
    "#                 continue\n",
    "#     except:\n",
    "#         continue\n",
    "# print(\"SUCCESS!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
