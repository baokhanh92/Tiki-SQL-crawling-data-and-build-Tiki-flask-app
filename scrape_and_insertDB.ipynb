{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "def load_website(url,prefix):\n",
    "    try:\n",
    "        response = requests.get(prefix+url)\n",
    "        return BeautifulSoup(response.text)\n",
    "    except Exception as err:\n",
    "        print(f'ERROR: {err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_insert(cat, j, articles, k, cur, conn, tablename):\n",
    "    #Works for scraping beautiful soup product item from tiki product page\n",
    "    try:\n",
    "        #scrape and assign to variables\n",
    "        images = articles[k].img['src']\n",
    "        fprice = articles[k].find_all(\"span\",{\"class\":\"final-price\"})[0].text.strip().split()[0]\n",
    "        rprice = articles[k].find_all(\"span\",{\"class\":\"price-regular\"})[0].text\n",
    "        discount = ['None' if len(articles[k].find_all(\"span\",{\"class\":\"final-price\"})[0].text.strip().split()) == 1 else articles[k].find_all(\"span\",{\"class\":\"final-price\"})[0].text.strip().split()[1]][0]\n",
    "        seller = articles[k]['data-brand']\n",
    "        titles = articles[k].a['title'].strip().replace('\\'','').replace('\"','')\n",
    "        subcategory = articles[k]['data-category'].strip()\n",
    "        category = cat[j][0]\n",
    "        num_reviews = [articles[k].find_all('p',{\"class\":'review'})[0].text.strip('\\(\\)') if articles[k].find_all('p',{\"class\":'review'}) != [] else 'Chưa có nhận xét'][0]\n",
    "        ratings = [articles[k].find_all('span',{\"class\":'rating-content'})[0].find('span')['style'].split(':')[1] if articles[k].find_all('span',{\"class\":'rating-content'}) != [] else 'Rating not available'][0]\n",
    "        tikinow = ['NO' if articles[k].find_all('i',{\"class\":\"tikicon icon-tikinow-20\"}) == [] else 'YES'][0]\n",
    "        productlink = articles[k].a['href']\n",
    "        #build query string\n",
    "        query = f\"\"\"INSERT INTO {tablename}(images, fprice, category, subcategory, titles, seller, rprice, discount, ratings, num_reviews, tikinow, productlink)\n",
    "                    VALUES('{images}','{fprice}', '{category}', '{subcategory}','{titles}','{seller}','{rprice}','{discount}','{ratings}','{num_reviews}','{tikinow}','{productlink}');\"\"\"\n",
    "        #commit to connection\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "    except Exception as err:\n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tiki home page\n",
    "soup = load_website('https://tiki.vn/',prefix='')\n",
    "#scrape the categories and their links and store in array\n",
    "categories = soup.find_all('a',{\"class\":'MenuItem__MenuLink-tii3xq-1 efuIbv'})\n",
    "category, link = [], []\n",
    "for h in range(len(categories)):\n",
    "    try:\n",
    "        link.append(categories[h]['href'])\n",
    "        category.append(categories[h].text)\n",
    "    except:\n",
    "        print('pass')\n",
    "cat = list(zip(category,link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create connect to DB and cursor\n",
    "conn = psycopg2.connect(\"dbname=thuctamdb user=postgres password=thuctam\")\n",
    "\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new table\n",
    "tablename = 'products'\n",
    "query = f'''\n",
    "CREATE TABLE {tablename}(\n",
    "   id SERIAL PRIMARY KEY,\n",
    "   images VARCHAR(1024),\n",
    "   fprice VARCHAR(1024),\n",
    "   category VARCHAR(1024),\n",
    "   subcategory VARCHAR(1024),\n",
    "   titles VARCHAR(1024),\n",
    "   seller VARCHAR(1024),\n",
    "   rprice VARCHAR(1024),\n",
    "   discount VARCHAR(1024),\n",
    "   ratings VARCHAR(1024),\n",
    "   num_reviews VARCHAR(1024),\n",
    "   tikinow VARCHAR(1024),\n",
    "   productlink VARCHAR(1024)\n",
    ");'''\n",
    "cur.execute(query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(cat)):\n",
    "    try:\n",
    "        soup = load_website(cat[j][1],prefix='')\n",
    "        articles = soup.find_all('div', {\"class\":'product-item'})\n",
    "        print('Reading '+cat[j][1])\n",
    "        for k in range(len(articles)):\n",
    "            scrape_and_insert(cat, j, articles, k, cur, conn, tablename)\n",
    "                \n",
    "        # Read next page cursor at the bottom of a product page        \n",
    "        links = soup.find_all('div',{\"class\":'list-pager'})  \n",
    "        \n",
    "        #While next page cursor is not empty, read next page cursor to move to next product page\n",
    "        while links[0].find_all('a', {\"class\": \"next\"}) != []:\n",
    "            try:\n",
    "                soup = load_website(links[0].find_all('a', {\"class\": \"next\"})[0]['href'],prefix='https://tiki.vn')\n",
    "                articles = soup.find_all('div', {\"class\":\"product-item\"})\n",
    "                print('Reading',cat[j][0],links[0].find_all('a', {\"class\": \"next\"})[0]['href'].split('&')[1],sep=' ')\n",
    "                for i in range(len(articles)):\n",
    "                    scrape_and_insert(cat, j, articles, i, cur, conn, tablename)\n",
    "                links = soup.find_all('div',{\"class\":'list-pager'})\n",
    "            except:\n",
    "                continue\n",
    "    except:\n",
    "        continue\n",
    "print(\"SUCCESS!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tablename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a16f55a28a40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'SELECT * FROM {tablename} ORDER BY id DESC LIMIT 5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tablename' is not defined"
     ]
    }
   ],
   "source": [
    "query = f'SELECT * FROM {tablename} ORDER BY id DESC LIMIT 5'\n",
    "cur.execute(query)\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
